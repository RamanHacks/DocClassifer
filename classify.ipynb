{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'wandb' is not recognized as an internal or external command,\noperable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wandb login f90bc50f940247a6f1e5b7574e124c960c4cece9"
   ]
  },
  {
   "source": [
    "# Boilerplate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#torch related imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# lightning related imports\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy\n",
    "from pytorch_lightning.callbacks import Callback\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# sklearn related imports\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.preprocessing import label_binarize"
   ]
  },
  {
   "source": [
    "## Config Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    NUM_EPOCHS = 10\n",
    "    LR = 0.00001\n",
    "    TRAIN_CNN = False\n",
    "    BATCH_SIZE = 32\n",
    "    PIN_MEMORY = True\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VALID_BATCH_SIZE = 8"
   ]
  },
  {
   "source": [
    "# Data Preparation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Preprocessing Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {'0': 'letter',\n",
    " '1': 'form',\n",
    " '10': 'budget',\n",
    " '11': 'invoice',\n",
    " '12': 'presentation',\n",
    " '13': 'questionnaire',\n",
    " '14': 'resume',\n",
    " '15': 'memo',\n",
    " '2': 'email',\n",
    " '3': 'handwritten',\n",
    " '4': 'advertisement',\n",
    " '5': 'scientific report',\n",
    " '6': 'scientific publication',\n",
    " '7': 'specification',\n",
    " '8': 'file folder',\n",
    " '9': 'news article'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(fname, options=None):\n",
    "    df = pd.read_csv(fname, sep=\" \", header=None)\n",
    "    df.columns = [\"img_name\", \"label\"]\n",
    "    df[\"img_name\"] = 'images/' + df[\"img_name\"]\n",
    "    if options:\n",
    "        df = df[df[\"label\"].isin(options)]\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Dataset Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataset(Dataset):\n",
    "    def __init__(self, annotation_df, transform=None):\n",
    "        self.annotations = annotation_df\n",
    "        self.transform = transform  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_id = self.annotations.iloc[index, 0]\n",
    "        img = Image.open(img_id).convert(\"RGB\")\n",
    "        y_label = torch.tensor(self.annotations.iloc[index, 1])\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, y_label)"
   ]
  },
  {
   "source": [
    "## Dataloader Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, train_df, val_df, test_df, train_batch, val_batch, pin_memory, options=None, transform=None):\n",
    "        super().__init__()\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.test_df = test_df\n",
    "        self.train_batch=train_batch\n",
    "        self.val_batch=val_batch\n",
    "        self.pin_memory = pin_memory\n",
    "        self.transform = transform\n",
    "        if options:\n",
    "            self.num_classes = len(options)\n",
    "        else:\n",
    "            self.num_classes = 16\n",
    "    \n",
    "    def setup(self):\n",
    "        self.train_dataset = DocDataset(self.train_df, self.transform)\n",
    "        self.val_dataset = DocDataset(self.val_df, self.transform)\n",
    "        self.test_dataset = DocDataset(self.test_df, self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch,\n",
    "            shuffle=True,\n",
    "            num_workers=8,\n",
    "            pin_memory = self.pin_memory\n",
    "        )\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch,\n",
    "            num_workers=8,\n",
    "            pin_memory = self.pin_memory\n",
    "        )\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.val_batch,\n",
    "            num_workers=8,\n",
    "            pin_memory = self.pin_memory\n",
    "        )"
   ]
  },
  {
   "source": [
    "## DataPrep Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options = [0,1,2,3,4,5,11,13]\n",
    "options = None\n",
    "train_df = get_df(r'labels/train.txt', options)\n",
    "val_df = get_df(r'labels/val.txt', options)\n",
    "test_df = get_df(r'labels/test.txt', options)\n",
    "\n",
    "transform = transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "data_module = DocDataModule(train_df, val_df, test_df, config.TRAIN_BATCH_SIZE, config.VALID_BATCH_SIZE, config.PIN_MEMORY, options, transform)\n",
    "data_module.setup()\n"
   ]
  },
  {
   "source": [
    "# Modelling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Modelling Class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, num_classes, learning_rate, train_CNN=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.train_CNN = train_CNN\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        self.loss = nn.NLLLoss()\n",
    "\n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        logits = self.softmax(self.dropout(self.relu(features))).squeeze(1)\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        \n",
    "        # training metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, logger=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True, logger=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "\n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        \n",
    "        # validation metrics\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = self.train_CNN\n",
    "        return optimizer"
   ]
  },
  {
   "source": [
    "## Callback class"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(\n",
    "   monitor='val_loss',\n",
    "   patience=3,\n",
    "   verbose=False,\n",
    "   mode='min'\n",
    ")\n",
    "\n",
    "class ImagePredictionLogger(Callback):\n",
    "    def __init__(self, val_samples, num_samples=32):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.val_imgs, self.val_labels = val_samples\n",
    "        \n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        # Bring the tensors to CPU\n",
    "        val_imgs = self.val_imgs.to(device=pl_module.device)\n",
    "        val_labels = self.val_labels.to(device=pl_module.device)\n",
    "        # Get model prediction\n",
    "        logits = pl_module(val_imgs)\n",
    "        preds = torch.argmax(logits, -1)\n",
    "        # Log the images as wandb Image\n",
    "        trainer.logger.experiment.log({\n",
    "            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n",
    "                           for x, pred, y in zip(val_imgs[:self.num_samples], \n",
    "                                                 preds[:self.num_samples], \n",
    "                                                 val_labels[:self.num_samples])]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CKPT_PATH = 'model/'\n",
    "MODEL_CKPT = 'model/model-{epoch:02d}-{val_loss:.2f}'\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    filepath=MODEL_CKPT ,\n",
    "    save_top_k=3,\n",
    "    mode='min')"
   ]
  },
  {
   "source": [
    "## Training Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_samples = next(iter(data_module.val_dataloader()))\n",
    "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
    "val_imgs.shape, val_labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init our model\n",
    "model = DocModel(data_module.num_classes, config.LR)\n",
    "\n",
    "# Initialize wandb logger\n",
    "wandb_logger = WandbLogger(project='DocClassifier', job_type='train')\n",
    "\n",
    "# Initialize a trainer\n",
    "trainer = pl.Trainer(max_epochs=config.NUM_EPOCHS,\n",
    "                     progress_bar_refresh_rate=20, \n",
    "                     logger=wandb_logger,\n",
    "                     callbacks=[early_stop_callback,\n",
    "                                ImagePredictionLogger(val_samples)],\n",
    "                     checkpoint_callback=checkpoint_callback)\n",
    "\n",
    "# Train the model ⚡🚅⚡\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Evaluate the model on the held out test set ⚡⚡\n",
    "trainer.test()"
   ]
  },
  {
   "source": [
    "# Prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prediction Statements"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}